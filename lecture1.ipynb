{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d34322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd76a0",
   "metadata": {},
   "source": [
    "# Cosmological inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a0b47",
   "metadata": {},
   "source": [
    "## History\n",
    "People have been interested in the Universe for a long time. It started small, when people were tracking planetary motions in the years BCE. The first estimate for the Earth radius happend already in 240 BCE by Eratosthenes. Although he used different distance units, common for the time (stadia), for which we don't have exact conversion rate to metric, he could estimate the Earth radius ~1-2% error compared to todays known value. \n",
    "\n",
    "Although the history starts long ago, the modern cosmology starts in the early 1900s. One key aspect is the Einstein's general relativity theory, which is also base for todays discoveries. But theory itself is not enough. Another very import aspect is the experiment. Einstein's theory wouldn't have been successful if Eddington wouldn't have proved it in 29th May 1919. Decade later it was proposed by Georges Lemaître and 2 years later confirmed by Edwin Hubble, that more recessional velocity of a galaxy depends on the distance to Earth. So it was the early 1900s when we started to have better theory and also more data about the Universe.\n",
    "\n",
    "Another big discovery was done by accident in the 1960s by Americans Arno Allan Penzias and Robert Woodrow Wilson. The discovery was made with a antenna made for detecting radio waves for another project. They saw, that signal was stronger than expected and ruled out possibilities of hardware malfunction and static noise. The signal kept \"noisy\". Later they were put into contact Robert H. Dicke, astrophysicists 60km away whos reasearch group had a plan to start detecting CMB signals. They estimated that the signal corresponds to 3.5 K background. Later measurements have measured CMB more precisely:\n",
    "* Cosmich Background Explorer (COBE) - discovery of anisotropies in the CMB, launched in 1989\n",
    "* Wilkinson Microwave Anisotropy Probe (WMAP) - more accurate measurement of CMB, launched in 2001\n",
    "* Planck sattellite - Latest, most accurete measurement (last data release in 2018) launched in 2009 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffd9ae",
   "metadata": {},
   "source": [
    "## Frequentist vs Bayesian\n",
    "\n",
    "There are two approaches for statistics: frequentist vs Bayesian. The main differenece of these two approaches comes down to how they view parameters to be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b8231",
   "metadata": {},
   "source": [
    "### **Frequentist approach**\n",
    "\n",
    "In frequentist approach the probability is defined as \n",
    "\n",
    "$$P(x) = \\lim_{n\\rightarrow \\infty} \\frac{n_x}{n}\\,.$$\n",
    "\n",
    "In other words, probability is a long term frequency of an event in repeated, independent trials.\n",
    "\n",
    "In frequentist approach the parameters of interest are fixed (our model parameters), but unknown. So the random process applies to data acquisition process. To estimate the parameters we must observed data. There are many tecniques to estimate the parameters, some more common are: method of moments, maximum likelihood estimation (MLE).\n",
    "\n",
    "The results of an analysis are typically:\n",
    " 1) Point estimates (with confidence intervals) - Example: Mean height = 170 cm, CI: 168-172cm\n",
    " 2) Binary (for hypothesis testing) rejecing null hypothesis or not: p-value=0.03 < significance level = 0.05 thus we can reject null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d94f7a",
   "metadata": {},
   "source": [
    "### **Bayesian approach**\n",
    "\n",
    "The key concept of Bayesian approach is Bayes' theorem\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\,.$$\n",
    "\n",
    "Here we talk about probability as \"degree of belief\" based on data and our previous belief. Compared to frequentist approach, we now introduce new concept - prior, which encodes this \"previous belief\".\n",
    "\n",
    "In Bayesian statistics the parameters are not fixed anymore are are treated as random variables, following some distribution. The data itself is now \"fixed\".\n",
    "\n",
    "Both approaches likelihood (quantity measuring how well model explains  observed data), which will be part of the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30459389",
   "metadata": {},
   "source": [
    "## Bayesian framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3956a",
   "metadata": {},
   "source": [
    "### **Bayes' Theorem:**\n",
    "\n",
    "As it's name says, Bayesian framework is based on Bayes' Theorem. Renaming some variables from the Bayes's Thereom more useful way we have now\n",
    "\n",
    "$$P(\\mathbf{\\theta}|d) = \\frac{P(d|\\mathbf{\\theta})P(\\mathbf{\\theta})}{P(d)}\\,.$$\n",
    "\n",
    "where $d$ - data, $\\theta$  - parameters of interest. In this formulation we have\n",
    "\n",
    "#### Likelihood **$P(d|\\mathbf{\\theta})$**: \n",
    "\n",
    "Measures how well the model parameters $\\mathbf{\\theta}$ are compatible with the observed data $d$. Likelihood itself is not a probability distribution and thus does not need to normalize to 1.\n",
    "\n",
    "Common likelihood functions:\n",
    " * Gaussian (of independent measurements)\n",
    " * Poisson (Discrete distribution with independent events)\n",
    " * Binomial (Binary observations with fixed probability)\n",
    "\n",
    " In the case of multiple likelihoods, we can calculate the joint (total) likelihood as \n",
    " $$\\mathcal{L}_{\\rm tot} = \\prod_{i=1}^{n} \\mathcal{L}_i$$\n",
    "\n",
    "\n",
    "#### Prior **$P(\\mathbf{\\theta})$**:\n",
    "\n",
    "This is unique to Bayesian inference. It's a probability distribution over the parameters reflecting our belief before seeing data. Selecting prior is very important and can change the results.\n",
    "\n",
    " * Informative prior\n",
    "   * We have knowledge of previous experiment(s) results and/or data.\n",
    "   * Can help MCMC sampling to be more efficient\n",
    "   * Posterior is heavily dependent on prior\n",
    " * Weakly informative prior\n",
    "   * Help to rule out impossible or extreme values\n",
    "   * Helps to stabilize estimates (in case of low data scenarios)\n",
    "   * Posterior is mostly dependent on data\n",
    " * Uninformative prior\n",
    "   * Posterior is dominated by likelihood (data)\n",
    "\n",
    "#### Evidence **$P(d)$**:\n",
    "\n",
    "Otherwise known as marginal likelihood. Evidence is normalisation factor, as we require and is often ignored\n",
    "$$\\int P(\\theta|d) d\\theta = 1\\,,$$\n",
    "we need that \n",
    "$$\\int P(d|\\theta)P(\\theta)d\\theta = P(d)\\,.$$\n",
    "Sometimes it is also said that it measures how well model predicts the data. Latter makes it possible to compare different models. Using Bayes' theorem again we can now show that\n",
    "\n",
    "$$\\frac{P(M_1 | d)}{P(M_2 | d)} = \\overbrace{\\frac{P(D | M_1)}{P(D | M_2)}}^{\\text{Bayes factor}\\ K} \\times \\frac{P(M_1)}{P(M_2)}$$\n",
    "where $M$ now represents a \"model\".\n",
    "\n",
    "To interpret the Bayes' factor $K$ value we can use Jeffrey's scale which says that \n",
    " * $K < 1 \\Rightarrow$ model $M_2$ is better\n",
    " * $1 \\leq K < 10^{1/2} \\Rightarrow$ model difference is barely worth mentioning\n",
    " * $10^{1/2} \\leq K < 1 \\Rightarrow$ model $M_1$ is substantially better\n",
    " * $10 \\leq K \\leq 100 \\Rightarrow$ model $M_1$ is strongly better\n",
    " * $100 < K \\Rightarrow$ model $M_1$ is decisevely better\n",
    "\n",
    "#### Posterior **$P(\\mathbf{\\theta} | d)$**:\n",
    "\n",
    "Mostly people are interested in the posterior distribution. This shows parameters' distribution after taking into account prior and seeing the data. Usually final result of the Bayesian inference. For the posterior we can later calculate:\n",
    " * Mean, variance\n",
    " * Credible interval - smallest interval that contains N% of data.\n",
    "   * For example 95% credible interval is smallest interval that contains 95% of distribution mass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8fa714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_credible_interval(mean, std, credible_interval=0.95):\n",
    "    assert credible_interval < 1, \"Credible interval must be smaller than 1.\"\n",
    "    plt.clf()\n",
    "    clear_output(wait=True)\n",
    "    f_log_gaussian = lambda _x: -(_x-mean)**2/(2*std**2)\n",
    "    \n",
    "    x = np.linspace(mean-5*std, mean+5*std, 10_000)\n",
    "    y = f_log_gaussian(x)\n",
    "    \n",
    "    alpha = 1-credible_interval\n",
    "    z = sc.stats.norm.ppf(1-alpha/2)\n",
    "    # plt.clf()\n",
    "    # plt.figure(figsize=(9, 3.5))\n",
    "    plt.plot(x, np.exp(y-max(y)))\n",
    "    plt.fill_between(x, np.exp(y-max(y)), alpha=0.6)\n",
    "    plt.plot([mean, mean], [0, 1], label=\"Mean\", color=\"red\")\n",
    "    plt.plot([mean-std, mean-std], [0, np.exp(f_log_gaussian(mean-std)-max(y))], color=\"green\")\n",
    "    plt.plot([mean+std, mean+std], [0, np.exp(f_log_gaussian(mean+std)-max(y))], color=\"green\", label=f\"$\\\\mu\\\\pm\\\\sigma$ (CI 68.27%)\")\n",
    "    plt.plot([mean-z*std, mean-z*std], [0, np.exp(f_log_gaussian(mean-z*std)-max(y))], color=\"black\", linestyle=\"--\")\n",
    "    plt.plot([mean+z*std, mean+z*std], [0, np.exp(f_log_gaussian(mean+z*std)-max(y))], color=\"black\", linestyle=\"--\", label=f\"Credible interval {credible_interval*100:.2f}%\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(r\"$\\propto p(x)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa3b962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ef7ee7ac3e43778c00064d344bff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatText(value=1.0, description='Mean', step=0.1), FloatText(value=1.0, description='St…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.gaussian_credible_interval(mean, std, credible_interval=0.95)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_slider = widgets.FloatText(value=1, min=0, max=5, step=0.1, description='Mean')\n",
    "std_slider = widgets.FloatText(value=1, min=0, max=5, step=0.1, description='Std')\n",
    "credible_interval_slider = widgets.BoundedFloatText(value=0.6827, min=0, max=0.9999, step=0.05, description='CI')\n",
    "display(widgets.interact(gaussian_credible_interval, mean=mean_slider, std=std_slider, credible_interval=credible_interval_slider))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad17040",
   "metadata": {},
   "source": [
    "### Example - Coin flip\n",
    "\n",
    "* What to look for?\n",
    "  * Strong infomative prior (change $\\alpha$ or $\\beta$ higher. The bigger the value the \"stronger\" the prior.)\n",
    "  * Non-informative prior ($\\alpha=\\beta=1$ is uniform prior)\n",
    "  * The more data we have\n",
    "    * the more posterior is influenced by the likelihood\n",
    "    * likelihood gets narrower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b733054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_distribution(theta, heads, tails):\n",
    "    return theta**heads*(1-theta)**tails\n",
    "\n",
    "def beta_distribution(theta, alpha, beta):\n",
    "    return theta**(alpha-1)*(1-theta)**(beta-1)\n",
    "\n",
    "def log_binomial_distribution(theta, heads, tails):\n",
    "    return heads*np.log(theta) + tails*np.log(1-theta)\n",
    "\n",
    "def log_beta_distribution(theta, alpha, beta):\n",
    "    return (alpha-1)*np.log(theta) + np.log(1-theta)*(beta-1)\n",
    "\n",
    "def plot_coin_bayes(n_heads, n_tails, alpha, beta, use_log=False):\n",
    "    \"\"\" Plot distribution functions (posterior, prior, likelihood) for coin flips, given number of heads and tails.\n",
    "    \n",
    "    Comments:\n",
    "     * alpha = beta = 1 => Uniform prior distribution  \n",
    "     * alpha > beta => Distribution more tilted to lower values, \"left\"\n",
    "     * alpha < beta => Distribution more tilted to higher values, \"right\"\n",
    "\n",
    "    Args:\n",
    "        n_heads (int): number of heads\n",
    "        n_tails (int): number of tails\n",
    "        alpha (float): Beta distribution parameter 1 for prior\n",
    "        beta (float): Beta distribution parameter 2 for prior\n",
    "        use_log (bool): If use logarithm of pdfs to calculate prior. likelihood and posterior\n",
    "    \"\"\"\n",
    "    if (alpha+beta > 750) or (n_heads+n_tails > 750):\n",
    "        print(\"It's advised to use_log=True\")\n",
    "    \n",
    "    theta = np.linspace(1e-8, 1-1e-8, 1000)\n",
    "\n",
    "    n_total = n_heads + n_tails\n",
    "    mean_prior = (alpha)/(alpha+beta)\n",
    "    mean_likelihood = n_heads/(n_total)\n",
    "    mean_posterior = (alpha + n_heads)/(n_total + alpha + beta)\n",
    "\n",
    "\n",
    "    if use_log:\n",
    "        log_dist_prior = log_beta_distribution(theta, alpha, beta)\n",
    "        log_dist_likelihood = log_binomial_distribution(theta, n_heads, n_tails)\n",
    "        log_dist_posterior = log_dist_prior+log_dist_likelihood\n",
    "        \n",
    "        dist_prior = np.exp(log_dist_prior-max(log_dist_prior))\n",
    "        dist_likelihood = np.exp(log_dist_likelihood-max(log_dist_likelihood))\n",
    "        dist_posterior = np.exp(log_dist_posterior-max(log_dist_posterior))\n",
    "        f_mean_prior = np.exp(log_beta_distribution(mean_prior, alpha, beta) - max(log_dist_prior))\n",
    "        f_mean_posterior = np.exp(log_beta_distribution(mean_posterior, alpha + n_heads, beta + n_tails) - max(log_dist_posterior))\n",
    "\n",
    "    else:\n",
    "        dist_prior = beta_distribution(theta, alpha, beta)\n",
    "        dist_likelihood = binomial_distribution(theta, n_heads, n_tails)\n",
    "        dist_posterior = dist_prior*dist_likelihood\n",
    "        f_mean_prior = beta_distribution(mean_prior, alpha, beta)/max(dist_prior)\n",
    "        f_mean_posterior = beta_distribution(mean_posterior, alpha + n_heads, beta + n_tails)/max(dist_posterior)\n",
    "\n",
    "    with plt.rc_context({\"font.size\": 13}):\n",
    "        fig = plt.figure(figsize=(13, 8))\n",
    "        plt.style.use(\"seaborn-v0_8-dark\")\n",
    "\n",
    "        # First row, first column\n",
    "        ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "        ax1.plot([mean_prior, mean_prior], [0, f_mean_prior], color=\"black\", linestyle=\"--\", label=f\"$\\\\mathbb{{E}}[p(\\\\theta)] = {mean_prior:.2f}$\")\n",
    "        ax1.plot(theta, dist_prior/max(dist_prior), color=\"green\", label=\"Prior\")\n",
    "        ax1.fill_between(theta, dist_prior/max(dist_prior), color=\"lightgreen\", alpha=0.6)\n",
    "        ax1.set_title(\"Prior\")\n",
    "\n",
    "        # First row, second column\n",
    "        ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "        ax2.plot([mean_likelihood, mean_likelihood], [0, 1], color=\"black\", linestyle=\"--\", label=f\"$\\\\mathbb{{E}}[p(d|\\\\theta)] = {mean_likelihood:.2f}$\")\n",
    "        ax2.plot(theta, dist_likelihood/max(dist_likelihood), color=\"blue\", label=\"Likelihood\")\n",
    "        ax2.fill_between(theta, dist_likelihood/max(dist_likelihood), color=\"lightblue\", alpha=0.6)\n",
    "        ax2.set_title(\"Likelihood\")\n",
    "\n",
    "        # Second row, span both columns\n",
    "        ax3 = plt.subplot2grid((2, 2), (1, 0), colspan=2)\n",
    "        ax3.plot([mean_posterior, mean_posterior], [0, f_mean_posterior], color=\"black\", linestyle=\"--\", label=f\"$\\\\mathbb{{E}}[p(\\\\theta|d)] = {mean_posterior:.2f}$\")\n",
    "        ax3.plot(theta, dist_posterior/max(dist_posterior), color=\"red\", label=\"Posterior\")\n",
    "\n",
    "        ax3.plot(theta, dist_prior/max(dist_prior), color=\"green\", label=\"Prior\", linestyle=\"--\")\n",
    "        ax3.plot(theta, dist_likelihood/max(dist_likelihood), color=\"blue\", label=\"Likelihood\", linestyle=\"--\")\n",
    "\n",
    "        ax3.fill_between(theta, dist_posterior/max(dist_posterior), color=\"salmon\", alpha=0.6)\n",
    "        ax3.set_title(\"Posterior\")\n",
    "        \n",
    "        for _ax in [ax1, ax2, ax3]:\n",
    "            _ax.legend(frameon=True)\n",
    "            _ax.grid()\n",
    "            _ax.set_xlim(0, 1)\n",
    "            _ax.set_xlabel(r\"$\\text{p(Head)}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb0af42-d95f-423a-8cab-d39087c3dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c11432dd18a40d5b1c0b522a877a665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=1, description='N heads', step=0), IntText(value=1, description='N tails',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_coin_bayes(n_heads, n_tails, alpha, beta, use_log=False)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heads_slider = widgets.IntText(value=1, min=0, max=1000, step=0.1, description='N heads')\n",
    "tails_slider = widgets.IntText(value=1, min=0, max=1000, step=0.1, description='N tails')\n",
    "alpha_slider = widgets.BoundedFloatText(value=1, min=1, max=1000, step=1, description=r'$\\alpha$')\n",
    "beta_slider = widgets.BoundedFloatText(value=1, min=1, max=1000, step=1, description=r'$\\beta$')\n",
    "use_log_checkbox = widgets.Checkbox(value=False, description='Use log porbabilities')\n",
    "display(widgets.interact(plot_coin_bayes, n_heads=heads_slider, n_tails=tails_slider, alpha=alpha_slider, beta=beta_slider, use_log=use_log_checkbox))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d439a",
   "metadata": {},
   "source": [
    "### **Marginalization**\n",
    "\n",
    "Marginalization is the process of summing/integrating out variables that we are not interested in.\n",
    "\n",
    "We had posterior \n",
    "$P(\\theta, D) = P(\\theta_1, \\theta_2, \\dots | D)\\,.$\n",
    "Let's say, we are not interested in the effects cause by nuisance parameters $\\theta_3$ and $\\theta_4$, which we have to use when calculating, but are not relevant to our theoretical model. This happens for example, if the parameters describe the measurement process itself. In Bayesian statistics it is possible to easily remove them from analysis by doing\n",
    "\n",
    "$$P(\\theta_1, \\theta_2, \\theta_5, \\dots | D) = \\int \\int P(\\theta_1, \\theta_2, \\dots | D) d\\theta_3 d\\theta_4\\,.$$\n",
    "\n",
    "This is usually done numerically and when we use results produced by MCMC it's even simpler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55193280",
   "metadata": {},
   "source": [
    "## Cosmological datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afecafa",
   "metadata": {},
   "source": [
    "* Run 1 [Purely late-time] - Pantheon+ (without SH0ES calibration) + DESI DR2;\n",
    "* Run 2 [Purely early-time] - CMB-SPA;\n",
    "* Run 3 [Combined scenario] - CMB-SPA + Pantheon+ (without SH0ES calibration) + DESI DR2;\n",
    "* Run 4 [Purely late-time] - Pantheon+ (with SH0ES calibration) + DESI DR2;\n",
    "* Run 5 [Combined scenario] - CMB-SPA + Pantheon+ (with SH0ES calibration) + DESI DR2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d64a1",
   "metadata": {},
   "source": [
    "### **CMB** - Early Universe\n",
    " * Encodes information about early universe\n",
    " * Independent parameters\n",
    "   * At the decoupling:\n",
    "     * $\\Omega_b$ - Baryion density parameter\n",
    "     * $\\Omega_m$ - Matter density parameter\n",
    "     * $A_s$ - amplitude of the scalar power spectrum\n",
    "     * $n_s$ - scalar spectral index\n",
    "   * Post-decoupling:\n",
    "     * $\\theta_{MC}$ - angular scale of the sound horizon (depends on $H_0$)\n",
    "     * $\\tau$ - reionization optical depth\n",
    "   * Model depentent (a few examples): \n",
    "     * $w$ - equation of state (eos)\n",
    "     * $m_{\\nu_i}$ - neutrino masses\n",
    "   * In principle different parameters can be selected as independent ($\\theta_{MC} \\rightarrow \\Omega_0, \\Omega_{\\Lambda0}$)\n",
    "   \n",
    "[More about CMB parameters from a lecture by Syksy Räsänen ](https://www.mv.helsinki.fi/home/syrasane/cosmo/lect2024_10.pdf)\n",
    "\n",
    "#### Planck 2018\n",
    " * Main goal was to measure CMB\n",
    " * Up to date most accurate CMB power spectrum measurement\n",
    "\n",
    "#### BICEP/Keck\n",
    " * Focus on detecting B-mode polarization in CMB \n",
    "   * Signature of primordial gravitational waves from inflation\n",
    "\n",
    "<figure  style=\"text-align: center;\">\n",
    "  <img src=\"images/CMB.jpg\" alt=\"CMB\" style=\"width: 75%; height: auto;\">\n",
    "  <figcaption>Picture of a CMB</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd530f1f",
   "metadata": {},
   "source": [
    "### **Supernova Measurements** - Late universe\n",
    "\n",
    "Type Ia Supernovaes are mainly from late universe ($z\\leq1$). We measure apparent magnitude which depends on the luminosity distance\n",
    "\n",
    "$$D_L = (1+z)\\frac{c}{H_0} \\int \\limits_{0}^{z} \\frac{dz'}{E(z')}\\,,\\ E(z) = \\sqrt{\\Omega_m(1+z)^3 + \\Omega_\\Lambda}\\,.$$\n",
    "For the moment we assume flat universe. Supernovae measurement by itself can't measure $H_0$ directly. If distances to Supernovaes are added we can also measure local $H_0$.\n",
    "\n",
    "**NB! Supernovae data and CMB measurements give different values for H_0! $\\Rightarrow$ Hubble tension**\n",
    "\n",
    "<figure  style=\"text-align: center;\">\n",
    "  <img src=\"images/HubbleTension.png\" alt=\"Hubble tension\" style=\"width: 75%; height: auto;\">\n",
    "  <figcaption>Picture of a Hubble tension <a href=https://arxiv.org/pdf/1907.10625>https://arxiv.org/pdf/1907.10625 </a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Some of the most known datasets:\n",
    " * DES (Dark Energy Survey)\n",
    "   * ~1800 Sn type Ia with z~0.02 to ~1\n",
    "   * [Cosmology results](https://arxiv.org/pdf/2401.02929) and [newer analysis](https://arxiv.org/pdf/2511.07517)\n",
    "   * Data: [Older dataset from Cobaya](https://github.com/CobayaSampler/sn_data/tree/master/DESY5), [Newer analysis method Cobaya](https://github.com/CobayaSampler/sn_data/tree/master/DES-Dovekie)\n",
    " * Pantheon+\n",
    "   * ~1500 Sn type Ia with z~0.03 to 2.3 \n",
    "   * [Dataset article](https://arxiv.org/pdf/2112.03863) and [Cosmology results](https://arxiv.org/pdf/2202.04077)\n",
    "   * Data: [GitHub Cobaya](https://github.com/CobayaSampler/sn_data/tree/master/PantheonPlus)\n",
    " * SH0ES (Supernova $H_0$ for the Equation of State)\n",
    "   * Started 2005 \n",
    "   * Main aim is to measure $H_0$ (local)\n",
    "   * Measures Cepheid variable stars and Type Ia supernovae\n",
    "   * Use distance ladder to measure distances\n",
    "   * [Article](https://arxiv.org/pdf/2112.04510)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ed4c0",
   "metadata": {},
   "source": [
    "### **Baryon Acoustic Oscillations (BAO)** - Late universe\n",
    "\n",
    "Experiments measuring large scale structure. Acts as a \"standard\" ruler. \n",
    "\n",
    "BAO was created near decoupling when electrons and photons were coupled. In this time gravity tried to pull the matter together but the radiation pressure pushed it apart. This created sound waves inside the plasma. After cooling the sound waves \"stopped\" and left imprints into the current matter distribution. From the theory we can calculate the sound horizon (how far the sound waves moved) and using results from Placnk (to get $\\omega_m$ and $\\omega_r$) we get that $$r_s \\approx 147\\ \\rm Mpc\\,,$$ which is also seen in the matter power spectrum measurments. \n",
    "\n",
    "<figure  style=\"text-align: center;\">\n",
    "  <img src=\"images/BAO_creation.png\" alt=\"Hubble tension\" style=\"width: 75%; height: auto;\">\n",
    "  <figcaption>Picture from: Daniel J. Eisenstein (Arizona U., Astron. Dept. - Steward Observ.), Hee-jong Seo (Arizona U., Astron. Dept. - Steward Observ.), Martin J. White (UC, Berkeley, Astron. Dept.), \"On the Robustness of the Acoustic Scale in the Low-Redshift Clustering of Matter\" <a href=https://arxiv.org/abs/astro-ph/0604361>https://arxiv.org/abs/astro-ph/0604361</a> </figcaption>\n",
    "</figure>\n",
    "More about [BAO](https://arxiv.org/pdf/0910.5224).\n",
    "\n",
    "* DESI (Dark Energy Spectroscopic Instrument)\n",
    "  * Data: [DESI Data documentation](https://data.desi.lbl.gov/doc/) and also from [GitHub Cobaya](https://github.com/CobayaSampler/bao_data/tree/master/desi_bao_dr2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
